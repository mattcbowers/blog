{
  
    
        "post0": {
            "title": "Hello World, and Why I'm Inspired to Start a Blog",
            "content": ". Well, I’ve been thinking about getting this blog started for months now. I guess a combination of inertia, up-front investment in blogging platform selection/setup, and spending a little too much time writing and rewriting the first content post has drawn out the period from initial inspiration to making the blog a reality. Needless to say, I’m pretty excited to finally get things going. . Before we dive headlong into the weeds of ML algorithms, statistical methods, and whatever I happen to be learning and teaching at the moment, I figured it would be good to articulate why I’ve felt inspired to get started blogging in the first place. Hopefully this will serve the dual purpose of clarifying my intentions and introducing a vastly underappreciated concept in data science that I hope to weave through the posts to come. . Learning . The initial inception about blogging probably originated from some comments about learning that Jeremy Howard makes in the Practical Deep Learning course from fastai. During one of the lectures, he mentions that it’s a great idea to start blogging. To paraphrase Jeremy: . The thing I really love about blogging is that it helps you learn; by writing things down, you synthesize your ideas. . Beautiful. That definitely rings true for me. I tend to take notes and play around with code when learning new concepts anyway. One of my key hypotheses about this blogging experiment is that making the effort to transform those notes into blog posts will help me learn more effectively. . Teaching . Ah, teaching. Yes, sometimes it’s that thing that takes time away from your research, forcing you to sit alone in a windowless room squinting at hand-written math on a fat stack of homework assignments. But sometimes it actually involves interacting with students, endeavoring to explain a concept, and watching them light up when they get it. The latter manifestation of teaching was one of my favorite things about grad school and academia in general. While I certainly still get to do some teaching as an industry data scientist, I could see myself returning to a more teaching-centric gig somewhere off in the future. Thus we have our second key hypothesis about the blogging experiment, that the writing will entertain my inclination to teach. . Contributing . Working in the field of data science today is a bit like standing in front of a massive complementary all-you-can-learn buffet. There is an abundance of free material out on the interwebs for learning pretty much anything in data science from hello world python tutorials to research papers on cutting-edge deep learning techniques. I’ve personally benefited from many a blog post that helped me unpack a new concept or get started using a new tool. And let’s not forget the gigantic cyber warehouse full of freely available open source software tools that volunteer developers have straight-up donated to humanity. . I realize that up to now, I’ve simply been consuming all of this free goodness without giving anything substantive back in return. Well then, it’s time to start evening the score. Which brings us to key hypothesis number three, that through these blog posts, I might be able to create something helpful, thereby being of service to a community that has freely given so much to me. . Live Long and Prosper, Blog . Phew, there it is, the original source of inspiration for this blogging experiment, and three reasons I think it might be a good idea. The astute reader will have noticed that these three assertions have been formulated as hypotheses which are to be tested in the laboratory of experience. And thus, we also have our first glimpse of the scientific method, an underrated concept that is going to help us put the science back in data science. . With that, blog, I christen thee, Random Realizations. . .",
            "url": "https://mattcbowers.github.io/blog/2020/11/22/hello_world.html",
            "relUrl": "/2020/11/22/hello_world.html",
            "date": " • Nov 22, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "pandas essentials for the tidyverse aficionado",
            "content": "You may have noticed that there are usually at least half a dozen ways to accomplish any given data transformation task in pandas. Having lots of options is great, because it gives us the freedom to adopt our own personal style. In this post we&#39;ll take a look at how we can establish a clean, readable, and easily understandable style for data transformation with pandas, inspired by R&#39;s tidyverse. . In their book, R for Data Science, Garrett Grolemund and Hadley Wickham describe five essential tasks for working with dataframes. . filtering rows based on data values | sorting rows based on data values | selecting columns by name | adding new columns based on the existing columns | creating grouped summaries of the dataset | We also need a way to build up more complex transformations by chaining these fundamental operations together. . Before we dive in, here&#39;s the TLDR on the R functions and the corresponding pandas methods I use to accomplish those tasks. . Here&#39;s some other text. . description dplyr pandas . filter rows based on data values | filter() | query() | . sort rows based on data values | arrange() | sort_values() | . select columns by name | select() | filter() | . add new columns based on the existing columns | mutate() | assign() | . create grouped summaries of the dataset | group_by() summarise() | groupby() apply() | . chain operations together | %&gt;% | . | . Imports and Data . import pandas as pd import numpy as np . We&#39;ll use the nycflights13 dataset which contains data on the 336,776 flights that departed from New York City in 2013. . flights = pd.read_csv(&#39;https://www.openintro.org/book/statdata/nycflights.csv&#39;) flights.head() . year month day dep_time dep_delay arr_time arr_delay carrier tailnum flight origin dest air_time distance hour minute . 0 2013 | 6 | 30 | 940 | 15 | 1216 | -4 | VX | N626VA | 407 | JFK | LAX | 313 | 2475 | 9 | 40 | . 1 2013 | 5 | 7 | 1657 | -3 | 2104 | 10 | DL | N3760C | 329 | JFK | SJU | 216 | 1598 | 16 | 57 | . 2 2013 | 12 | 8 | 859 | -1 | 1238 | 11 | DL | N712TW | 422 | JFK | LAX | 376 | 2475 | 8 | 59 | . 3 2013 | 5 | 14 | 1841 | -4 | 2122 | -34 | DL | N914DL | 2391 | JFK | TPA | 135 | 1005 | 18 | 41 | . 4 2013 | 7 | 21 | 1102 | -3 | 1230 | -8 | 9E | N823AY | 3652 | LGA | ORF | 50 | 296 | 11 | 2 | . Select rows based on their values with query() . query() lets you retain a subset of rows based on the values of the data; it&#39;s like dplyr::filter() in R or WHERE in SQL. Its argument is a string specifying the condition to be met for rows to be included in the result. You specify the condition as an expression involving the column names and comparison operators like &lt;, &gt;, &lt;=, &gt;=, == (equal), and ~= (not equal). You can specify compound expressions using and and or, and you can even check if the column value matches any items in a list. . # compare one column to a value flights.query(&#39;month == 6&#39;) # compare two column values flights.query(&#39;arr_delay &gt; dep_delay&#39;) # using arithmetic flights.query(&#39;arr_delay &gt; 0.5 * air_time&#39;) # using &quot;and&quot; flights.query(&#39;month == 6 and day == 1&#39;) # using &quot;or&quot; flights.query(&#39;origin == &quot;JFK&quot; or dest == &quot;JFK&quot;&#39;) # column value matching any item in a list flights.query(&#39;carrier in [&quot;AA&quot;, &quot;UA&quot;]&#39;) . You may have noticed that it seems to be much more popular to filter pandas data frames using boolean indexing. Indeed when I ask my favorite search engine how to filter a pandas dataframe on its values, I find this tutorial, this blog post, various questions on Stack Overflow, and even the pandas documentation, all espousing boolean indexing. Here&#39;s what it looks like. . # canonical boolean indexing flights[(flights[&#39;carrier&#39;] == &quot;AA&quot;) &amp; (flights[&#39;origin&#39;] == &quot;JFK&quot;)] # the equivalent use of query() flights.query(&#39;carrier == &quot;AA&quot; and origin == &quot;JFK&quot;&#39;) . There are a few reasons I prefer query() over boolean indexing. . query() does not require me to type the dataframe name again, whereas boolean indexing requires me to type it every time I wish to refer to a column. | query() makes the code easier to read and understand, especially when expressions get complex. | query() is more computationally efficient than boolean indexing. | query() can safely be used in dot chains, which we&#39;ll see very soon. | Select columns by name with filter() . filter() lets you pick out a specific set of columns by name; it&#39;s analogous to dplyr::select() in R or SELECT in SQL. You can either provide exactly the column names you want, or you can grab all columns whose names contain a given substring or which match a given regular expression. This isn&#39;t a big deal when your dataframe has only a few columns, but is particularly useful when you have a dataframe with tens or hundreds of columns. . # select a list of columns flights.filter([&#39;origin&#39;, &#39;dest&#39;]) # select columns containing a particular substring flights.filter(like=&#39;time&#39;) # select columns matching a regular expression flights.filter(regex=&#39;e$&#39;) . Sort rows with sort_values() . sort_values() changes the order of the rows based on the data values; it&#39;s likedplyr::arrange() in R or ORDER BY in SQL. You can specify one or more columns on which to sort, where their order denotes the sorting priority. You can also specify whether to sort in ascending or descending order. . # sort by a single column flights.sort_values(&#39;air_time&#39;) # sort by a single column in descending order flights.sort_values(&#39;air_time&#39;, ascending=False) # sort by carrier, then within carrier, sort by descending distance flights.sort_values([&#39;carrier&#39;, &#39;distance&#39;], ascending=[True, False]) . Add new columns with assign() . assign() adds new columns which can be functions of the existing columns; it&#39;s like dplyr::mutate() from R. . # add a new column based on other columns flights.assign(speed = lambda x: x.distance / x.air_time) # another new column based on existing columns flights.assign(gain = lambda x: x.dep_delay - x.arr_delay) . If you&#39;re like me, this way of using assign() might seem a little strange at first. Let&#39;s break it down. In the call to assign() the keyword argument speed tells pandas the name of our new column. The business to the right of the = is a inline lambda function that takes the dataframe we passed to assign() and returns the column we want to add. . I like using x as the lambda argument because its easy to type and it evokes tabular data (think design matrix), which reminds me that it refers to the entire dataframe. We can then access the other columns in our dataframe using the dot like x.other_column. . It&#39;s true that you can skip the whole lambda business and refer to the dataframe to which you are assigning directly inside the assign. That might look like this. . flights.assign(speed = flights.distance / flights.air_time) . I prefer using a lambda for the following reasons. . If you gave your dataframe a good name, using the lambda will save you from typing the name every time you want to refer to a column. | The lambda makes your code more portable. Since you refer to the dataframe as a generic x, you can reuse this same assignment code on a dataframe with a different name. | Most importantly, the lambda will allow you to harness the power of dot chaining. | Chain transformations together with the dot chain . One of the awesome things about pandas is that the object.method() paradigm lets us easily build up complex dataframe transformations from a sequence of method calls. In R, this is effectively accomplished by the pipe %&gt;% operator. For example, suppose we want to look at high-speed flights from JFK to Honolulu, which would require us to query for JFK to Honolulu flights, assign a speed column, and maybe sort on that new speed column. . We can say: . # neatly chain method calls together ( flights .query(&#39;origin == &quot;JFK&quot;&#39;) .query(&#39;dest == &quot;HNL&quot;&#39;) .assign(speed = lambda x: x.distance / x.air_time) .sort_values(by=&#39;speed&#39;, ascending=False) .query(&#39;speed &gt; 8.0&#39;) ) . We compose the dot chain by wrapping the entire expression in parentheses and indenting each line within. The first line is the name of the dataframe on which we are operating. Each subsequent line has a single method call. . There are a few great things about writing the code this way: . Readability. It&#39;s easy to scan down the left margin of the code to see what&#39;s happening. The first line gives us our noun (the dataframe) and each subsequent line starts with a verb. You could read this as &quot;take flights then query the rows where origin is JFK, then query for rows where destination is HNL, then assign a new column called speed, then sort the dataframe by speed, then query only for the rows where speed is greater than 8.0. | Flexibility - It&#39;s easy to comment out individual lines and re-run the cell. It&#39;s also easy to reorder operations, since only one thing happens on each line. | Neatness - We have not polluted our workspace with any intermediate variables, nor have we wasted any mental energy thinking of names for any temporary variables. | By default, dot chains do not modify the original dataframe; they just output a temporary result that we can inspect directly in the output. If you want to store the result, or pass it along to another function (e.g. for plotting), you can simply assign the entire dot chain to a variable. . # sotre the output of the dot chain in a new dataframe flights_high_speed = ( flights .assign(speed = lambda x: x.distance / x.air_time) .query(&#39;speed &gt; 8.0&#39;) ) . Collapsing rows into grouped summaries with groupby() . groupby() combined with apply() gives us flexibility and control over our grouped summaries; it&#39;s like dplyr::group_by() and dplyr::summarise() in R. This is the primary pattern I use for SQL-style groupby operations in pandas. Specifically it unlocks the following essential functionality you&#39;re used to having in SQL. . specify the names of the aggregation columns we create | specify which aggregation function to use on which columns | compose more complex aggregations such as the proportion of rows meeting some condition | aggregate over arbitrary functions of multiple columns | Let&#39;s check out the departure delay stats for each carrier. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;n_flights&#39;: len(d), &#39;med_delay&#39;: d.dep_delay.median(), &#39;avg_delay&#39;: d.dep_delay.mean(), })) .head() ) . n_flights med_delay avg_delay . carrier . 9E 1696.0 | -1.0 | 17.285967 | . AA 3188.0 | -2.0 | 9.142409 | . AS 66.0 | -4.5 | 5.181818 | . B6 5376.0 | -1.0 | 13.137091 | . DL 4751.0 | -2.0 | 8.529573 | . While you might be used to apply() acting over the rows or columns of a dataframe, here we&#39;re calling apply on a grouped dataframe object, so it&#39;s acting over the groups. According to the pandas documentation: . The function passed to apply must take a dataframe as its first argument and return a dataframe, a series or a scalar. apply will then take care of combining the results back together into a single dataframe or series. apply is therefore a highly flexible grouping method. . We need to supply apply() with a function that takes each chunk of the grouped dataframe and returns (in our case) a series object with one element for each new aggregation column. Notice that I use a lambda to specify the function we pass to apply()), and that I name its argument d, which reminds me that it&#39;s a dataframe. My lambda returns a pandas series whose index entries specify the new aggregation column names, and whose values constitute the results of the aggregations for each group. Pandas will then stitch everything back together into a lovely dataframe. . Notice how nice the code looks when we use this pattern. Each aggregation is specified on its own line, which makes it easy to see what aggregation columns we&#39;re creating and allows us to comment, uncomment, and reorder the aggregations without breaking anything. . Here are some more complex aggregations to illustrate some useful patterns. . ( flights .groupby([&#39;carrier&#39;]) .apply(lambda d: pd.Series({ &#39;avg_gain&#39;: np.mean(d.dep_delay - d.arr_delay), &#39;pct_delay_gt_30&#39;: np.mean(d.dep_delay &gt; 30), &#39;pct_late_dep_early_arr&#39;: np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)), &#39;avg_arr_given_dep_delay_gt_0&#39;: d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean(), &#39;cor_arr_delay_dep_delay&#39;: np.corrcoef(d.dep_delay, d.arr_delay)[0,1], })) .head() ) . avg_gain pct_delay_gt_30 pct_late_dep_early_arr avg_arr_given_dep_delay_gt_0 cor_arr_delay_dep_delay . carrier . 9E 9.247642 | 0.196934 | 0.110259 | 39.086111 | 0.932485 | . AA 7.743726 | 0.113237 | 0.105395 | 30.087165 | 0.891013 | . AS 16.515152 | 0.106061 | 0.121212 | 28.058824 | 0.864565 | . B6 3.411458 | 0.160528 | 0.084449 | 37.306866 | 0.914180 | . DL 7.622816 | 0.097874 | 0.100821 | 30.078029 | 0.899327 | . Here&#39;s what&#39;s happening. . np.mean(d.dep_delay - d.arr_delay) aggregates over the difference of two columns. | np.mean(d.dep_delay &gt; 30) computes the proportion of rows where the delay is greater than 30 minutes. Generating a boolean series based on some condition and then using mean() to find the proportion comes up all the time. | np.mean((d.dep_delay &gt; 0) &amp; (d.arr_delay &lt; 0)) shows that we can compute proportions where conditions on multiple columns are met. | d.query(&#39;dep_delay &gt; 0&#39;).arr_delay.mean() computes the average arrival delay on flights where the departure was delayed. Here we first filter each grouped dataframe down to the subset of rows where departure delay is greater than zero using query(), and then we take the mean of the remaining arrival delays. | np.corrcoef(d.dep_delay, d.arr_delay)[0,1] computes the correlation coefficient between departure and arrival delays. Remember we can use pretty much any reduction operation to collapse values down to a scalar. | . You might have noticed that the canonical pandas approach for grouped summaries is to use agg(). That works well if you need to apply the same aggregation function on each column in the dataframe, e.g. taking the mean of every column. But because of the kind of data I work with these days, it&#39;s much more common for me to use customized aggregations like those above, so the groupby() apply() idiom works best for me. . Wrapping Up . There you have it, how to pull off the five most essential data transformation tasks using pandas in a style that evokes the elegance of the tidyverse. Remember that part of the beauty of pandas is that since there are so many ways to do most tasks, you can develop your own style based on the kind of data you work with, what you like about other tools, how you see others using the tools, and of course your own taste and preferences. . So, what&#39;s your pandas style? .",
            "url": "https://mattcbowers.github.io/blog/pandas/tidyverse/2020/11/08/pandas_for_the_dplyr_user.html",
            "relUrl": "/pandas/tidyverse/2020/11/08/pandas_for_the_dplyr_user.html",
            "date": " • Nov 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mattcbowers.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://mattcbowers.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Well howdy! I’m Matt Bowers. I’m currently a data scientist at Uber where I specialize in using statistical modeling and machine learning to make large-scale telematics data useful. Before Uber I earned an MS in Applied Statistics and a PhD in Atmospheric Sciences at Purdue University where I studied the spatial-temporal correlation structure of tropical convection systems and developed statistical methods for climate research. I was also an Insight Data Science fellow at Silicon Valley in the summer of 2017. .",
          "url": "https://mattcbowers.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mattcbowers.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}